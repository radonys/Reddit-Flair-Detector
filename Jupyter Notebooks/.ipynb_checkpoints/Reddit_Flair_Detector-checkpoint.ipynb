{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/radonys/Reddit-Flair-Detector/blob/master/Reddit_Flair_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTIVuFgiuoAn"
   },
   "source": [
    "## Reddit Flair Detector\n",
    "\n",
    "A Reddit Flair Detector using Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByC5RpkXvVNY"
   },
   "source": [
    "### Install Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZVX18CvvTUI"
   },
   "outputs": [],
   "source": [
    "!pip install praw pandas numpy scikit-learn matplotlib scipy PyDrive\n",
    "!pip install gensim nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQ5D5ZVHxuCW"
   },
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2Zx5yYjxxwg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cE4GI4Odi3TE"
   },
   "source": [
    "### Variable Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztsmWwu5i8WZ"
   },
   "outputs": [],
   "source": [
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"AMA\"]\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUPnSWuedSF2"
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLYT1shRdVgY"
   },
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "def string_form(value):\n",
    "    return str(value)\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wi9NhpmXyxd7"
   },
   "source": [
    "### Getting Reddit India Data\n",
    "\n",
    "Using PRAW module [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juhA_Mxiy59Z"
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='_4Z0SOGEI31a3Q', client_secret='fzhJE6_cjPqLN6goG4R7cJ6SK3Q', user_agent='reddit-flair-detector', username='radonys', password='reddit_india')\n",
    "\n",
    "subreddit = reddit.subreddit('india')\n",
    "topics_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
    "\n",
    "for flair in flairs:\n",
    "  \n",
    "  get_subreddits = subreddit.search(flair, limit=100)\n",
    "  \n",
    "  for submission in get_subreddits:\n",
    "    \n",
    "    topics_dict[\"flair\"].append(flair)\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"id\"].append(submission.id)\n",
    "    topics_dict[\"url\"].append(submission.url)\n",
    "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "    topics_dict[\"created\"].append(submission.created)\n",
    "    topics_dict[\"body\"].append(submission.selftext)\n",
    "    topics_dict[\"author\"].append(submission.author)\n",
    "    \n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "      comment = comment + ' ' + top_level_comment.body\n",
    "    topics_dict[\"comments\"].append(comment)\n",
    "    \n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "_timestamp = topics_data[\"created\"].apply(get_date)\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)\n",
    "del topics_data['created']\n",
    "topics_data.to_csv('reddit-india-data.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Px4vwjjHZAl9"
   },
   "source": [
    "### Load Reddit India Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zWwOiwWZGEg"
   },
   "outputs": [],
   "source": [
    "topics_data = pd.read_csv('reddit-india-data.csv')\n",
    "topics_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6STtfu_CPHf"
   },
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wliPfSFCMfg"
   },
   "outputs": [],
   "source": [
    "topics_data['title'] = topics_data['title'].apply(string_form)\n",
    "topics_data['body'] = topics_data['body'].apply(string_form)\n",
    "topics_data['comments'] = topics_data['comments'].apply(string_form)\n",
    "\n",
    "topics_data['title'] = topics_data['title'].apply(clean_text)\n",
    "topics_data['body'] = topics_data['body'].apply(clean_text)\n",
    "topics_data['comments'] = topics_data['comments'].apply(clean_text)\n",
    "\n",
    "feature_combine = topics_data[\"title\"] + topics_data[\"comments\"] + topics_data[\"url\"]\n",
    "topics_data = topics_data.assign(feature_combine = feature_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9jX8iPMHtbw"
   },
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXc_hCoxHw_M"
   },
   "outputs": [],
   "source": [
    "def nb_classifier(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "  nb = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "  nb.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = nb.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51DbsDDwIhXR"
   },
   "source": [
    "### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUl5A1VHIhry"
   },
   "outputs": [],
   "source": [
    "def linear_svm(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "  sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "                 ])\n",
    "  sgd.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = sgd.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNHhGkgGIwak"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ea_qJsuYIxWe"
   },
   "outputs": [],
   "source": [
    "def logisticreg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                 ])\n",
    "  logreg.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = logreg.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxYj_jzhk-Bk"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfW7k_a_lAV4"
   },
   "outputs": [],
   "source": [
    "def randomforest(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  \n",
    "  ranfor = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators = 1000, random_state = 42)),\n",
    "                 ])\n",
    "  ranfor.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = ranfor.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMKLKsblnjM5"
   },
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xzMs1A5nls3"
   },
   "outputs": [],
   "source": [
    "def mlpclassifier(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "  \n",
    "  mlp = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MLPClassifier(hidden_layer_sizes=(30,30,30))),\n",
    "                 ])\n",
    "  mlp.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = mlp.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeiX8Q8AMaLb"
   },
   "source": [
    "### Train Test Varied Data ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyANdHpzMe52"
   },
   "outputs": [],
   "source": [
    "def train_test(X,y):\n",
    " \n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "  print(\"Results of Naive Bayes Classifier\")\n",
    "  nb_classifier(X_train, X_test, y_train, y_test)\n",
    "  print(\"Results of Linear Support Vector Machine\")\n",
    "  linear_svm(X_train, X_test, y_train, y_test)\n",
    "  print(\"Results of Logistic Regression\")\n",
    "  logisticreg(X_train, X_test, y_train, y_test)\n",
    "  print(\"Results of Random Forest\")\n",
    "  randomforest(X_train, X_test, y_train, y_test)\n",
    "  print(\"Results of MLP Classifier\")\n",
    "  mlpclassifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kbm5FWYmHanw"
   },
   "source": [
    "### Train-Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVQk224eHdsL"
   },
   "outputs": [],
   "source": [
    "cat = topics_data.flair\n",
    "\n",
    "V = topics_data.feature_combine\n",
    "W = topics_data.comments\n",
    "X = topics_data.title\n",
    "Y = topics_data.body\n",
    "Z = topics_data.url\n",
    "\n",
    "print(\"Flair Detection using Title as Feature\")\n",
    "#train_test(X,cat)\n",
    "print(\"Flair Detection using Body as Feature\")\n",
    "#train_test(Y,cat)\n",
    "print(\"Flair Detection using URL as Feature\")\n",
    "#train_test(Z,cat)\n",
    "print(\"Flair Detection using Comments as Feature\")\n",
    "train_test(V,cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3LQb-vv7G9c"
   },
   "source": [
    "### References\n",
    "\n",
    "1) http://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "\n",
    "2) https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ByC5RpkXvVNY",
    "WQ5D5ZVHxuCW",
    "FUPnSWuedSF2"
   ],
   "include_colab_link": true,
   "name": "Reddit Flair Detector.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
